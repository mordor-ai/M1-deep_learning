{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Neural_net.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM82/Rx9bPGbTEQsSpiSgYK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Petrichoeur/Neural_Net_from_scratch/blob/master/Neural_net_class/Simple_to_use_Neural_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pERZ0nKzsJNX",
        "colab_type": "text"
      },
      "source": [
        "## Import "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTmwbnGGxKQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    import numpy as np \n",
        "    import pandas as pd  \n",
        "    from collections import OrderedDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtuMXKxWsMMu",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmIzvIFNxLzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu_prime(x): \n",
        "    ''' Useful function for later \n",
        "     All derivative are kinda easy , but Relu derivative is not well fit for numpy wise simple calculation , \n",
        "     so i have to huse a pre_made function to make it easier ''' \n",
        "    tmp=x\n",
        "    tmp[x<=0] = 0\n",
        "    tmp[x>0] = 1\n",
        "    return tmp \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DenseLayer() : \n",
        "\n",
        "    def __init__(self,size,activation='tanh',out=False): \n",
        "        self.out= out # For make some prediction later\n",
        "        self.size = size   # Size of the layer\n",
        "        self.activation=activation # Type of activation\n",
        "        self.weights=None  # Weights\n",
        "        self.weights_shape=None # Input_shape of weights\n",
        "        self.delta=None   # Delta for backward pass\n",
        "        self.output=None  # Output activation for the current layer \n",
        "        self.bias=None   # Bias \n",
        "        self.derivative=None # Derivative of the output activation\n",
        "    def weight_init(self):\n",
        "        self.weights=2*np.random.rand(self.weights_shape,self.size)-1  # Init the weights beetween [-1,1]\n",
        "        self.bias=np.ones((1,self.size)) \n",
        "\n",
        "    def Output(self,input_):  \n",
        "        '''Input_ = Input_ from the previous layer for forward pass    \n",
        "           At each pass the output is calculated but the derivative of the output too  \n",
        "           It's a way of having always the specific activation and derivative function for each layer. \n",
        "             '''\n",
        "        self.function=self.activation  # Wich type of activation we will use \n",
        "        #self.output= 1/(1+np.exp(-(np.dot(input_,self.weights)+self.bias))).reshape(1,self.size) \n",
        "        self.dot_activation=np.dot(input_,self.weights)+self.bias # We get the input dot the weights with add of the bias \n",
        "\n",
        "        if self.function=='tanh' :    # Hyperbolique tangente \n",
        "            self.output=np.tanh(self.dot_activation)  \n",
        "            self.derivative= (1-self.output**2)\n",
        "        elif self.function =='sigmoid' : # Sigmoid\n",
        "            self.output = 1/(1+np.exp(-self.dot_activation))  \n",
        "            self.derivative = self.output*(1-self.output)\n",
        "        elif self.function == 'Relu' :  # Rectified Linear Unit\n",
        "            self.output= np.maximum(self.dot_activation,0)  \n",
        "            self.derivative=relu_prime(self.output)\n",
        "        elif self.function == 'Lecun_tanh':   # Lecun hyperbolic tangente activation \n",
        "            self.output= 1.7159*np.tanh((2/3)*self.dot_activation)\n",
        "            self.derivative =1.7159*(2/3)*(1-((self.output / 1.7159)**2)) \n",
        "        elif self.function=='nothing': \n",
        "            self.output = self.dot_activation \n",
        "            self.derivative= 1  \n",
        "        else :  \n",
        "             raise ValueError('Activation function unknown')\n",
        "         \n",
        "\n",
        "\n",
        "class NeuralNet():   # Class for Neural Net , It initialize the neural net when call .\n",
        "    def __init__(self,input_shape,batch_size=1): \n",
        "        self.input_shape=input_shape # Shape of the input\n",
        "        self.batch_size = 1  #Batch size for further implementation of Mini Batch Descend and other optimizer than SGD\n",
        "        self.layers={}  # A dic of Layer, better than list for memory use \n",
        "        self.input_layer=np.empty((1,input_shape)) # Input_layer size .\n",
        "        self.nn_size=0   # Number of Layers, Input doesn't count.\n",
        "        self.layer_shape=[input_shape] # Shape of each layer \n",
        "        self.forward_count=0 # To keep the count , just in case \n",
        "        self.pred=None  # To get the output after a forward pass\n",
        "        self.output_error=None # The error to minimize \n",
        "    def add(self,layer): \n",
        "        self.layers[self.nn_size]=layer   # We add a layer\n",
        "        self.layers[self.nn_size].weights_shape=self.layer_shape[self.nn_size] # We initialize the shape to match other layers \n",
        "        self.layers[self.nn_size].weight_init() # We initialize the weights .\n",
        "        self.layer_shape.append(self.layers[self.nn_size].size) # We keep the size for next layer use .\n",
        "        self.nn_size += 1  # We are getting bigger !!!!\n",
        "    \n",
        "    def forward(self,X,return_=False): \n",
        "        self.forward_count += 1 # Keep the count ! \n",
        "        self.input_layer=X # First input layer is the data to use \n",
        "        self.layers=OrderedDict(self.layers.items()) # You don't have to, but it's a way to preventing some index mistakes.\n",
        "        for i in range(self.nn_size) :   # Loop for passing the information through the network \n",
        "            if i==0:\n",
        "                self.layers[i].Output(self.input_layer)  # We calculate the first output\n",
        "            else : \n",
        "                self.layers[i].Output(self.layers[i-1].output) # We calculate the output depending the previous output \n",
        "        self.pred=self.layers[self.nn_size-1].output   # The last outputs, aka the prediction.\n",
        "        if return_ == True :  # If you want to keep only the prediction\n",
        "            return  self.pred\n",
        "\n",
        "    def Backward(self,y,X_train,alpha=0.2): \n",
        "        ''' FOr this function i won't go into mathematics details because you can easily find all the \n",
        "        maths you need on backward pass , gradient descend and weights updates. All you have to remember is : \n",
        "         for each layer , the delta is :  \n",
        "                Layer_error =(delta of the next layer dot the weights of the actual layer ) \n",
        "              (Not exactly but i use this for  my algorithm)  Layer _ delta =   Layer_error *( the derivative of the output of the previous layer )  \n",
        "              ( It eventually do the same stuff as basic SGD, but i keep the bias of the previous layer in each layer , so i have to do it differently)\n",
        "                update_on_the_weights_of_the_actual_layer == The weights + learning_rate*Previous_layers_output*Next_layers_delta  \n",
        "                \n",
        "                '''\n",
        "        self.X_train = X_train # Crazy train on the station !!! The data we will use for backward pass\n",
        "        self.output_error= y-self.pred  # The error on prediction.\n",
        "        self.output_delta=self.output_error*self.layers[self.nn_size-1].derivative # The delta of the output !!\n",
        "        for i in range(self.nn_size-1,0,-1):  # Reversed range , we start from the end\n",
        "            if i==self.nn_size-1:\n",
        "                self.layers[i].error=self.output_delta.dot(self.layers[i].weights.T)  #First layer error is special, output_delta it is \n",
        "            else : \n",
        "                self.layers[i].error=self.layers[i+1].delta.dot(self.layers[i].weights.T) #others layers errors act the same , mainstream layers .\n",
        "            self.layers[i].delta=self.layers[i].error*self.layers[i-1].derivative  # We get the delta values for each layers\n",
        "        for i in range(self.nn_size-1) :  \n",
        "            if i ==0 : \n",
        "                self.layers[i].weights += alpha*self.X_train.T.dot(self.layers[i+1].delta) # Update the weights , special because it's the input\n",
        "                self.layers[i].bias += alpha*self.layers[i+1].delta # Update the bias , special too\n",
        "            elif i ==self.nn_size-1 :  \n",
        "                self.layers[i].weights +=alpha*self.layers[i-1].output.T.dot(self.output_delta) # Update the weights, special because \n",
        "                                                                                                            #it's the end of the backwardpass journey\n",
        "                self.layers[i].bias += alpha*self.output_delta # Update the bias \n",
        "            else : \n",
        "                self.layers[i].weights += alpha*self.layers[i-1].output.T.dot(self.layers[i+1].delta)  # Update the weights\n",
        "                self.layers[i].bias += alpha*self.layers[i+1].delta # Update the bias \n",
        "\n",
        "    def train(self,y,X,epoch=5):  # Okay so , we can train now\n",
        "        for _ in range(epoch): # Number of epochs or how many time the entire train set is being used for minimizing the error .\n",
        "            for idx,el in enumerate(X): \n",
        "                el=el.reshape(1,X.shape[1]) # Reshape, i don't want shape errors .\n",
        "                self.forward(el) # One pass forward\n",
        "                self.Backward(y[idx],el) # One pass backward  \n",
        "\n",
        "\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHGPa6FLxWzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = NeuralNet(2) # Input_size = 2 \n",
        "test.add(DenseLayer(15,activation='Lecun_tanh')) # I want to try a Lecun activation Function ! \n",
        "test.add(DenseLayer(10,activation='Lecun_tanh'))# Lecun_tanh is great \n",
        "test.add(DenseLayer(1,activation='Lecun_tanh',out=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0DKSz8vnbnd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_classification # for generating a data set \n",
        "from sklearn.model_selection import train_test_split # For getting validation data and shuffling the data too\n",
        "X, Y = make_classification(n_samples=500 ,n_features=2, n_redundant=0, n_informative=2,\n",
        "                             n_classes=2)  # we get X with features and Y with label\n",
        "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.3) # Splitting for train and test "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_ZgFBnsE96d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "test.train(y_train,x_train,epoch=30)  # Let's train our data !! "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_Q4QmE8E-Zk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "ae74f9fd-b0e1-4f2d-b890-d94b7fff2c13"
      },
      "source": [
        "\n",
        "testy=[] \n",
        "for i in range(len(x_test)):\n",
        "    prediction= test.forward(x_test[i],return_=True)\n",
        "    testy.append(prediction) \n",
        "\n",
        "from sklearn.metrics import classification_report \n",
        "print(classification_report(np.round(testy).reshape(150),y_test))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        -0.0       0.96      0.90      0.93        73\n",
            "         1.0       0.91      0.96      0.94        77\n",
            "\n",
            "    accuracy                           0.93       150\n",
            "   macro avg       0.94      0.93      0.93       150\n",
            "weighted avg       0.93      0.93      0.93       150\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU7IwT1LV9Z1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Okay so we are pretty good on the test !!!!  We can try some other activation function : \n",
        "test2 = NeuralNet(2) # Input_size = 2 \n",
        "test2.add(DenseLayer(15,activation='Relu')) # RELU !!\n",
        "test2.add(DenseLayer(10,activation='tanh'))# TANH !!\n",
        "test2.add(DenseLayer(1,activation='sigmoid',out=True)) # SIGMMOOIIDDD !!!!! "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlfO3VtlV93K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "test2.train(y_train,x_train,epoch=30)  # Let's train our data !! "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFcdvgPhWAw9",
        "colab_type": "code",
        "outputId": "6ddd715a-206d-4388-8e82-bd28dde1f42e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "\n",
        "testy2=[] \n",
        "for i in range(len(x_test)):\n",
        "    prediction= test2.forward(x_test[i],return_=True)\n",
        "    testy2.append(prediction) \n",
        "\n",
        "from sklearn.metrics import classification_report \n",
        "print(classification_report(np.round(testy2).reshape(150),y_test)) # It's slightly better "
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.94      0.95        70\n",
            "         1.0       0.95      0.96      0.96        80\n",
            "\n",
            "    accuracy                           0.95       150\n",
            "   macro avg       0.95      0.95      0.95       150\n",
            "weighted avg       0.95      0.95      0.95       150\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvzIm7OAFtAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZS3H2cmk0vz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeglJN4_k_rV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqu9_ieM0qdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaEGk94I04ef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru8pzqwWRsTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
