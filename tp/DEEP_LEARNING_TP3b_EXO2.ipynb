{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_input, n_neurons,  bias=None):\n",
    "        self.weights =  ((np.random.rand(n_input, n_neurons)-0.5)*1)#.astype(int)\n",
    "        self.weights = self.weights.astype(float)\n",
    "        self.bias = np.zeros((n_neurons)) \n",
    "        self.last_activation = None\n",
    "        self.delta = None\n",
    "        print (\"IN : \", n_input, \"  x \", n_neurons)\n",
    "\n",
    "    def activate(self, x):\n",
    "        self.last_activation = np.tanh(np.dot(x, self.weights) + self.bias)\n",
    "        print (x)\n",
    "        print ('dot')\n",
    "        print (self.weights)\n",
    "        print ('=')\n",
    "        print (np.dot(x, self.weights))\n",
    "        print (\"Activation =>\", self.last_activation)\n",
    "        return self.last_activation\n",
    "\n",
    "    def apply_activation_derivative(self, r):\n",
    "        return 1 - r ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self._layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self._layers.append(layer)\n",
    "\n",
    "    def feed_forward(self, X):\n",
    "        for layer in self._layers:\n",
    "            X = layer.activate(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(\"=================================================\")? (<ipython-input-7-0583ee041e7c>, line 82)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-0583ee041e7c>\"\u001b[0;36m, line \u001b[0;32m82\u001b[0m\n\u001b[0;31m    print \"=================================================\"\u001b[0m\n\u001b[0m                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(\"=================================================\")?\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self._layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self._layers.append(layer)\n",
    "\n",
    "    def feed_forward(self, X):\n",
    "        for layer in self._layers:\n",
    "            X = layer.activate(X)\n",
    "        return X\n",
    "\n",
    "    def backpropagation(self, X, y, learning_rate):\n",
    "\n",
    "        output = self.feed_forward(X)\n",
    "\n",
    "        for i in reversed(range(len(self._layers))):\n",
    "            layer = self._layers[i]\n",
    "\n",
    "            if layer == self._layers[-1]:\n",
    "                layer.delta = (y - output) * layer.apply_activation_derivative(output)\n",
    "                print ('------------', i, '-----------------')\n",
    "                print (layer.delta)\n",
    "                print ('------------', i, '-----------------')\n",
    "            else:\n",
    "                next_layer = self._layers[i + 1]\n",
    "                layer.delta = (np.dot(next_layer.weights, next_layer.delta)) * layer.apply_activation_derivative(layer.last_activation)\n",
    "                print ('------------', i, '-----------------')\n",
    "                print ('w next', next_layer.weights)\n",
    "                print (\"dot\")\n",
    "                print ('delta    next', next_layer.delta)\n",
    "                print (\"=\")\n",
    "                print (np.dot(next_layer.weights, next_layer.delta))\n",
    "                print (\"*\")\n",
    "                print ('act', layer.apply_activation_derivative(layer.last_activation))\n",
    "                print (\"=\")\n",
    "                print ('d', layer.delta)\n",
    "                print ('------------', i, '-----------------')\n",
    "\n",
    "        print ''\n",
    "        print ''\n",
    "        for i in range(len(self._layers)):\n",
    "            layer = self._layers[i]\n",
    "            if i==0:\n",
    "                in_=X\n",
    "            else:\n",
    "                in_ = self._layers[i - 1].last_activation\n",
    "            in_=np.expand_dims(in_, axis=0)\n",
    "            print ('------------', i, '-----------------')\n",
    "            print ('layer.weights', layer.weights)\n",
    "            print (\"---\")\n",
    "            print ('d', layer.delta)\n",
    "            print (\"*lr*\")\n",
    "            print (in_.T)\n",
    "            print (\"=\")\n",
    "            print ('', layer.delta * in_.T * learning_rate)\n",
    "            print (\"----\")\n",
    "            layer.weights += layer.delta * in_.T * learning_rate\n",
    "\n",
    "            print ('layer.weights', layer.weights, \"     (\",np.sum(np.abs(layer.delta * in_.T)),\")\")\n",
    "            print ('------------', i, '-----------------')\n",
    "\n",
    "\n",
    "            #layer.weights += layer.delta * in_.T * learning_rate\n",
    "            #layer.bias    += layer.delta * learning_rate\n",
    "\n",
    "    def test(self):\n",
    "        acc = 0.0\n",
    "        #Base de test\n",
    "        for j in range(len(Xt)):\n",
    "            p = self.feed_forward(Xt[j])\n",
    "            if p*yt[j]>0:\n",
    "                acc+=1\n",
    "        print acc/float(Xt.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        self.backpropagation(np.array([-1, 1, -2]), 1.0, 0.1)\n",
    "        print \"=================================================\"\n",
    "        print \"=================================================\"\n",
    "        print \"=================================================\"\n",
    "        print \"=================================================\"\n",
    "        self.backpropagation(np.array([-1, 1, -2]), 1.0, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    nn = NeuralNetwork()\n",
    "    nn.add_layer(Layer(3, 2))\n",
    "    nn.add_layer(Layer(2, 4))\n",
    "    nn.add_layer(Layer(4, 1))\n",
    "\n",
    "    nn.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
