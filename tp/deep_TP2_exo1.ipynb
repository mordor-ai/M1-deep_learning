{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging as log\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, n_input, n_neuron, bias=None):\n",
    "        self.weights   = np.random.rand(n_input, n_neurons)\n",
    "        self.bias = np.zeros((n_neurons))\n",
    "\n",
    "    def activate(self, x):\n",
    "        self.last_activation = np.tanh(np.dot(x, self.weights) + self.bias)\n",
    "        return self.last_activation\n",
    "\n",
    "    def apply_activation_derivation(self, r):\n",
    "        return 1 - r ** 2\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self._layers.append(layer)\n",
    "\n",
    "    def feed_forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.activate(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, n_input, n_neuron, bias=None):\n",
    "        self.weights   = np.random.rand(n_input, n_neurons)\n",
    "        self.bias = np.zeros((n_neurons))\n",
    "        self.last_activation = Nome\n",
    "        self.delta=  None\n",
    "\n",
    "    def activate(self, x):\n",
    "        self.last_activation = np.tanh(np.dot(x, self.weights) + self.bias)\n",
    "        return self.last_activation\n",
    "\n",
    "    def apply_activation_derivation(self, r):\n",
    "        return 1 - r ** 2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Network:\n",
    "   # def __init__(self, data = None):\n",
    "    def __init__ ( self ,verbose=0):\n",
    "        self._layers = []\n",
    "        # initializing verbose mode\n",
    "        if (self.verbose!=0 ):\n",
    "            log.basicConfig(format=\"%(levelname)s: %(message)s\", level=log.DEBUG)\n",
    "            log.info(\"Verbose output.\")\n",
    "        else:\n",
    "            log.basicConfig(format=\"%(levelname)s: %(message)s\")\n",
    "            # somme log messages examples \n",
    "            #log.info(\"This should be verbose.\")\n",
    "            #log.warning(\"This is a warning.\")\n",
    "            #log.error(\"This is an error.\")\n",
    "        \n",
    "    # function for training data set\n",
    "    def add_layer(self, layer):\n",
    "        log.error(\"not implemented\")   \n",
    "        raise NotImplementedError()    \n",
    "    def feed_forward(self,X):\n",
    "        log.error(\"not implemented\")   \n",
    "        raise NotImplementedError()\n",
    "    def backpropagation(self):\n",
    "        log.error(\"not implemented\")   \n",
    "        raise NotImplementedError()\n",
    "    def train(self):\n",
    "        log.error(\"not implemented\")   \n",
    "        raise NotImplementedError()\n",
    "    def test(self):\n",
    "        log.error(\"not implemented\")   \n",
    "        raise NotImplementedError()\n",
    "        \n",
    "class NetworkMat(Network):\n",
    "    def_init(self):\n",
    "        super.__init__()\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        log.info( \"starting add_layer\")\n",
    "        self.layers.append(layer)\n",
    "    def feed_forward(self,X): \n",
    "        for layer in self._layers:\n",
    "                X = layer.activate(X)\n",
    "        return X\n",
    "    def backpropagation():\n",
    "        \n",
    "\n",
    "class NetworkInd(Network):\n",
    "        def_init(self):\n",
    "            super.__init__()\n",
    "    \n",
    "def backpropagation(self, X,y, learning_rate): \n",
    "\n",
    "    output =  self.feed_forward(X)\n",
    "    for i in reversed(range(len(self._layers)))\n",
    "        layer = self._layers[i]\n",
    "        if layer ==  self._layers[i]:\n",
    "            layer.delta=  self._layers[i+1]\n",
    "            layer.delta = (np.dot(next_layer.weights, next.layer.delta))* layer.apply_activation_derivative(layer.last_activation)\n",
    "for i in range(len(self._layers))  :\n",
    "    layer=  self._layers[i]\n",
    "    if i==0:\n",
    "            in_=X\n",
    "    else\n",
    "        in_=  self.Layers[i-1].last_activation\n",
    "    in_= np.expand_dims(in_,axis=0)\n",
    "    layer.weignts += layer.delta*in_.T* learning_rate\n",
    "    layer.bias += layer.delta* learning_rate\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Vu par Marine Pc à 17:09Vu par Arthur Gicquiaud à 17:11Vu par Sylvain Povia à 17:11\n",
    "#Jeremy Juventin\n",
    "\n",
    "\n",
    "def backpropagation(self, X, y, learning_rate):\n",
    "\n",
    "        output = self.feed_forward(X)\n",
    "\n",
    "        for i in reversed(range(len(self._layers))):\n",
    "            layer = self._layers[i]\n",
    "\n",
    "            if layer == self._layers[-1]:\n",
    "                layer.delta = (y - output) * layer.apply_activation_derivation(output)\n",
    "            else:\n",
    "                next_layer = self._layers[i + 1]\n",
    "                layer.delta = (np.dot(next_layer.weights, next_layer.delta)) * layer.apply_activation_derivation(layer.last_activation)\n",
    "\n",
    "            for i in range(len(self._layers))\n",
    "                layer = self._layers[i]\n",
    "\n",
    "                if i==0:\n",
    "                    in_ = X\n",
    "                else:\n",
    "                    in_ = self._layers[i-1].last_activation\n",
    "                in_=np.expand_dims(in_, axis=0)\n",
    "                layer.weights += layer.delta * in_.T * learning_rate\n",
    "                layer.bias += layer.delta * learning_rate\n",
    "\n",
    "    def test(self):\n",
    "        #TODO\n",
    "\n",
    "    def train(self, X, y, Xt, yt, learning_rate, epoch=400):\n",
    "        self.Xt = Xt\n",
    "        self.yt = yt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
